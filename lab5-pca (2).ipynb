{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab5-pca.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTransformerRegistry.enable('default')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from sklearn.decomposition import PCA\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Principal components\n",
    "\n",
    "There are many perspectives on principal components analysis (PCA). PCA is variously described as: a dimension reduction method; a method of approximating covariance structure; a latent model; a change of basis that optimally describes covariation; and so on. How can these seemingly distinct views be compatible with a single method?\n",
    "\n",
    "A simple answer is that PCA has a very wide range of applications in which it serves different purposes. Sometimes it is applied to find a few derived variables based on a large number of input variables -- hence, 'dimension reduction'. At others, it is used to interpret covariation among many variables -- hence, a 'covariance approximation'. With different objectives come different perspectives. \n",
    "\n",
    "In PSTAT100, we'll try to look beyond this and focus on the core technique of PCA: *finding linear data transformations that preserve variation and covariation*. In short, we'll focus on the *principal components* (PC) part of PCA, taking the following view.\n",
    "* **Principal components** are _**linear data transformations**_. \n",
    "* The **analysis** of principal components is _**varied depending on the application**_.\n",
    "\n",
    "We'll keep an open mind for the time being about what the analysis (A) part of PCA entails. So, what does it mean to say that 'principal components are linear data transformations'? Suppose you have a dataset with $n$ observations and $p$ variables. As a dataframe, this might look something like the following:\n",
    "\n",
    "Observation | Variable 1 | Variable 2 | $\\cdots$ | Variable $p$\n",
    "---|---|---|---|---\n",
    "1 | $x_{11}$ | $x_{12}$ | $\\cdots$ | $x_{1p}$\n",
    "2 | $x_{21}$ | $x_{22}$ | $\\cdots$ | $x_{2p}$\n",
    "$\\vdots$ | $\\vdots$ | $\\vdots$ |  | $\\vdots$\n",
    "$n$ | $x_{n1}$ | $x_{n2}$ | $\\cdots$ | $x_{np}$\n",
    "\n",
    "We can represent the values as a data matrix $\\mathbf{X}$ with $n$ rows and $p$ columns:\n",
    "\n",
    "$$\\mathbf{X} \n",
    "= \\underbrace{\\left[\\begin{array}{cccc}\n",
    "    \\mathbf{x}_1 &\\mathbf{x}_2 &\\cdots &\\mathbf{x}_p\n",
    "    \\end{array}\\right]}_{\\text{column vectors}}\n",
    "= \\left[\\begin{array}{cccc}\n",
    "    x_{11} &x_{12} &\\cdots &x_{1p} \\\\\n",
    "    x_{21} &x_{22} &\\cdots &x_{2p} \\\\\n",
    "    \\vdots &\\vdots &\\ddots &\\vdots \\\\\n",
    "    x_{n1} &x_{n2} &\\cdots &x_{np}\n",
    "\\end{array}\\right]$$\n",
    "\n",
    "To say that the principal components are linear data transformations means that each principal component is of the form:\n",
    "\n",
    "$$\\text{PC} = \\mathbf{Xv} = v_1 \\mathbf{x}_1 + v_2 \\mathbf{x}_2 + \\cdots + v_p \\mathbf{x}_p$$\n",
    "\n",
    "In other words, a linear combination of the columns of the data matrix. In PCA, the linear combination coefficients are known as *loadings*; the PC loadings are found in a particular way using the correlations among the columns.\n",
    "\n",
    "---\n",
    "### Objectives\n",
    "\n",
    "In this lab, you'll focus on computing and interpreting principal components:\n",
    "* finding the loadings (linear combination coefficients) for each PC;\n",
    "* quantifying the variation captured by each PC;\n",
    "* visualization-based techniques for selecting a number of PC's to A(nalyze);\n",
    "* plotting and interpreting loadings. \n",
    "\n",
    "In addition, you'll encounter a few ways that PCA is useful in exploratory analysis:\n",
    "* describing variation and covariation;\n",
    "* identifying variables that 'drive' variation and covariation;\n",
    "* visualizing multivariate data.\n",
    "\n",
    "You'll work with a selection of county summaries from the 2010 U.S. census. The first few rows of the dataset are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>Women</th>\n",
       "      <th>White</th>\n",
       "      <th>Citizen</th>\n",
       "      <th>IncomePerCap</th>\n",
       "      <th>Poverty</th>\n",
       "      <th>ChildPoverty</th>\n",
       "      <th>Professional</th>\n",
       "      <th>Service</th>\n",
       "      <th>...</th>\n",
       "      <th>Transit</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Minority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>51.567339</td>\n",
       "      <td>75.788227</td>\n",
       "      <td>73.749117</td>\n",
       "      <td>24974.49970</td>\n",
       "      <td>12.912305</td>\n",
       "      <td>18.707580</td>\n",
       "      <td>32.790974</td>\n",
       "      <td>17.170444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095259</td>\n",
       "      <td>1.305969</td>\n",
       "      <td>1.835653</td>\n",
       "      <td>26.500165</td>\n",
       "      <td>43.436374</td>\n",
       "      <td>73.736490</td>\n",
       "      <td>5.433254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.733726</td>\n",
       "      <td>22.536870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>51.151337</td>\n",
       "      <td>83.102616</td>\n",
       "      <td>75.694057</td>\n",
       "      <td>27316.83516</td>\n",
       "      <td>13.424230</td>\n",
       "      <td>19.484305</td>\n",
       "      <td>32.729943</td>\n",
       "      <td>17.950921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126621</td>\n",
       "      <td>1.443800</td>\n",
       "      <td>3.850477</td>\n",
       "      <td>26.322179</td>\n",
       "      <td>44.051127</td>\n",
       "      <td>81.282655</td>\n",
       "      <td>5.909353</td>\n",
       "      <td>0.363327</td>\n",
       "      <td>7.589820</td>\n",
       "      <td>15.214260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>46.171840</td>\n",
       "      <td>46.231594</td>\n",
       "      <td>76.912223</td>\n",
       "      <td>16824.21643</td>\n",
       "      <td>26.505629</td>\n",
       "      <td>43.559621</td>\n",
       "      <td>26.124042</td>\n",
       "      <td>16.463434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.495403</td>\n",
       "      <td>1.621725</td>\n",
       "      <td>1.501946</td>\n",
       "      <td>24.518283</td>\n",
       "      <td>31.921135</td>\n",
       "      <td>71.594256</td>\n",
       "      <td>7.149837</td>\n",
       "      <td>0.089774</td>\n",
       "      <td>17.525557</td>\n",
       "      <td>51.943818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Bibb</td>\n",
       "      <td>46.589099</td>\n",
       "      <td>74.499889</td>\n",
       "      <td>77.397806</td>\n",
       "      <td>18430.99031</td>\n",
       "      <td>16.603747</td>\n",
       "      <td>27.197085</td>\n",
       "      <td>21.590099</td>\n",
       "      <td>17.955450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.503137</td>\n",
       "      <td>1.562095</td>\n",
       "      <td>0.731468</td>\n",
       "      <td>28.714391</td>\n",
       "      <td>36.692621</td>\n",
       "      <td>76.743846</td>\n",
       "      <td>6.637936</td>\n",
       "      <td>0.394151</td>\n",
       "      <td>8.163104</td>\n",
       "      <td>24.165971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Blount</td>\n",
       "      <td>50.594351</td>\n",
       "      <td>87.853854</td>\n",
       "      <td>73.375498</td>\n",
       "      <td>20532.27467</td>\n",
       "      <td>16.721518</td>\n",
       "      <td>26.857377</td>\n",
       "      <td>28.529302</td>\n",
       "      <td>13.942519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362632</td>\n",
       "      <td>0.419941</td>\n",
       "      <td>2.265413</td>\n",
       "      <td>34.844893</td>\n",
       "      <td>38.449142</td>\n",
       "      <td>81.826708</td>\n",
       "      <td>4.228716</td>\n",
       "      <td>0.356493</td>\n",
       "      <td>7.699640</td>\n",
       "      <td>10.594744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     State   County      Women      White    Citizen  IncomePerCap    Poverty  \\\n",
       "0  Alabama  Autauga  51.567339  75.788227  73.749117   24974.49970  12.912305   \n",
       "1  Alabama  Baldwin  51.151337  83.102616  75.694057   27316.83516  13.424230   \n",
       "2  Alabama  Barbour  46.171840  46.231594  76.912223   16824.21643  26.505629   \n",
       "3  Alabama     Bibb  46.589099  74.499889  77.397806   18430.99031  16.603747   \n",
       "4  Alabama   Blount  50.594351  87.853854  73.375498   20532.27467  16.721518   \n",
       "\n",
       "   ChildPoverty  Professional    Service  ...   Transit  OtherTransp  \\\n",
       "0     18.707580     32.790974  17.170444  ...  0.095259     1.305969   \n",
       "1     19.484305     32.729943  17.950921  ...  0.126621     1.443800   \n",
       "2     43.559621     26.124042  16.463434  ...  0.495403     1.621725   \n",
       "3     27.197085     21.590099  17.955450  ...  0.503137     1.562095   \n",
       "4     26.857377     28.529302  13.942519  ...  0.362632     0.419941   \n",
       "\n",
       "   WorkAtHome  MeanCommute   Employed  PrivateWork  SelfEmployed  FamilyWork  \\\n",
       "0    1.835653    26.500165  43.436374    73.736490      5.433254    0.000000   \n",
       "1    3.850477    26.322179  44.051127    81.282655      5.909353    0.363327   \n",
       "2    1.501946    24.518283  31.921135    71.594256      7.149837    0.089774   \n",
       "3    0.731468    28.714391  36.692621    76.743846      6.637936    0.394151   \n",
       "4    2.265413    34.844893  38.449142    81.826708      4.228716    0.356493   \n",
       "\n",
       "   Unemployment   Minority  \n",
       "0      7.733726  22.536870  \n",
       "1      7.589820  15.214260  \n",
       "2     17.525557  51.943818  \n",
       "3      8.163104  24.165971  \n",
       "4      7.699640  10.594744  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tidy county-level 2010 census data\n",
    "census = pd.read_csv('data/census2010.csv', encoding = 'latin1')\n",
    "census.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observational units are U.S. counties, and each row (observation) is a county. The values are, for the most part, percentages of the county population. You can find variable descriptions in the metadata file in the data directory (*data > census2010metadata.csv*). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 0. Correlations\n",
    "\n",
    "PCA identifies variable combinations that capture covariation by decomposing the correlation matrix. So, to start with, let's examine the correlation matrix for the 2010 county-level census data to get a sense of which variables tend to vary together.\n",
    "\n",
    "The correlation matrix is a matrix of all pairwise correlations between variables. If $x_ij$ denotes the value for the $i$th observation of variable $j$, then the entry at row $j$ and column $k$ of the correlation matrix $\\mathbf{R}$ is:\n",
    "\n",
    "$$r_{jk} = \\frac{\\sum_i (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k)}{S_j S_k}$$\n",
    "\n",
    "In the census data, the `State` and `County` columns indicate the geographic region for each observation; essentially, they are a row index. So we'll drop them before computing the matrix $\\mathbf{R}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store quantitative variables separately\n",
    "x_mx = census.drop(columns = ['State', 'County'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, the matrix is simple to compute in pandas using `.corr()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "corr_mx = x_mx.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix can be inspected directly to determine which variables vary together. For example, we could look at the correlations of the percentage of the population that is employed with all other variables in the dataset by extracting the `Employed` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChildPoverty   -0.744510\n",
       "Poverty        -0.735569\n",
       "Unemployment   -0.697985\n",
       "Minority       -0.439053\n",
       "Service        -0.403261\n",
       "MeanCommute    -0.252111\n",
       "Drive          -0.215038\n",
       "Carpool        -0.144336\n",
       "Production     -0.136277\n",
       "Citizen        -0.087343\n",
       "Office         -0.014838\n",
       "OtherTransp    -0.010041\n",
       "FamilyWork      0.055654\n",
       "Women           0.131181\n",
       "Transit         0.151700\n",
       "SelfEmployed    0.154107\n",
       "PrivateWork     0.264826\n",
       "WorkAtHome      0.303839\n",
       "White           0.432856\n",
       "Professional    0.473413\n",
       "IncomePerCap    0.767001\n",
       "Employed        1.000000\n",
       "Name: Employed, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlation between poverty and other variables\n",
    "corr_mx.loc[:, 'Employed'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that correlation is a number in the interval [-1, 1] whose magnitude indicates the strength of the relationship between variables. \n",
    "* Correlations near -1 are *strongly negative*, and mean that the variables *tend to vary in opposition* \n",
    "    + (large values of one coincide with small values of the other and vice-versa).\n",
    "* Correlations near 1 are *strongly positive*, and mean that the variables *tend to vary together*\n",
    "    + (large values coincide and small values coincide).\n",
    "\n",
    "As a result, from examining these entries, it can be seen that the percentage of the county population that is employed is:\n",
    "* strongly *negatively* correlated with child poverty, poverty, and unemployment, meaning it *tends to vary in opposition* with these variables;\n",
    "* strongly *positively* correlated with income per capita, meaning it *tends to vary together* with this variable.\n",
    "\n",
    "If instead we wanted to look up the correlation between just two variables, we could retrieve the relevant entry directly using `.loc[...]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7670009685702536"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlation between employment and income per capita\n",
    "corr_mx.loc['Employed', 'IncomePerCap']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So across U.S. counties employment is, perhaps unsurprisingly, strongly and positively correlated with income per capita, meaning that higher employment rates tend to coincide with higher incomes per capita."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 0 (a)\n",
    "\n",
    "Check your understanding by repeating this for a different pair of variables.\n",
    "\n",
    "#### (i) Find the correlation between the poverty rate and demographic minority rate and store it in `pov_dem_rate`. \n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q0_a_i\n",
    "points: 1\n",
    "manual: false\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6231625196890354"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlation between poverty and percent minority\n",
    "pov_dem_rate = corr_mx.loc['Poverty', 'Minority']\n",
    "# print\n",
    "pov_dem_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Test q0_a_i does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/92/xt0krj_94d3g33fkk4pl8h_00000gn/T/ipykernel_99040/3663484926.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"q0_a_i\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/otter/check/utils.py\u001b[0m in \u001b[0;36mevent_logger\u001b[0;34m(wrapped, self, args, kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/otter/check/utils.py\u001b[0m in \u001b[0;36mevent_logger\u001b[0;34m(wrapped, self, args, kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEventType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHECK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                 \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshelve_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/otter/check/notebook.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(self, question, global_env)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \"\"\"\n\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Running check for question: {question}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         test_path, test_name = resolve_test_info(\n\u001b[0m\u001b[1;32m    202\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve_nb_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfail_silently\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/otter/check/utils.py\u001b[0m in \u001b[0;36mresolve_test_info\u001b[0;34m(tests_dir, nb_path, tests_url_prefix, question)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtests_dir\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtests_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtests_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test {question} does not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mtest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtests_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Test q0_a_i does not exist"
     ]
    }
   ],
   "source": [
    "grader.check(\"q0_a_i\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (ii) Interpret the correlation: is it large or small, positive or negative, and what does that mean?\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q0_a_ii\n",
    "points: 1\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The correlation in defiantly large and it is a positive relation._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "While direct inspection is useful, it can be cumbersome to check correlations for a large number of variables this way. A heatmap -- a colored image of the matrix -- provides a (sometimes) convenient way to see what's going on without having to examine the numerical values directly. The cell below shows one way of constructing this plot.\n",
    "\n",
    "Notice that the color scale shows positive correlations in orange, negative ones in blue, strong correlations in dark tones, and weak correlations in light tones. This is known as a 'diverging color gradient', and should, as a rule of thumb, always be used for plots of this type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt corr_mx\n",
    "corr_mx_long = corr_mx.reset_index().rename(\n",
    "    columns = {'index': 'row'}\n",
    ").melt(\n",
    "    id_vars = 'row',\n",
    "    var_name = 'col',\n",
    "    value_name = 'Correlation'\n",
    ")\n",
    "\n",
    "# construct plot\n",
    "alt.Chart(corr_mx_long).mark_rect().encode(\n",
    "    x = alt.X('col', title = '', sort = {'field': 'Correlation', 'order': 'ascending'}), \n",
    "    y = alt.Y('row', title = '', sort = {'field': 'Correlation', 'order': 'ascending'}),\n",
    "    color = alt.Color('Correlation', \n",
    "                      scale = alt.Scale(scheme = 'blueorange', # diverging gradient\n",
    "                                        domain = (-1, 1), # ensure white = 0\n",
    "                                        type = 'sqrt'), # adjust gradient scale\n",
    "                     legend = alt.Legend(tickCount = 5)) # add ticks to colorbar at 0.5 for reference\n",
    ").properties(width = 300, height = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 0 (b)\n",
    "\n",
    "Which variable is self employment rate most *positively* correlated with? Refer to the heatmap.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q0_b\n",
    "points: 1\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "*Working at home.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## 1. Principal components analysis\n",
    "\n",
    "Principal components analysis (PCA) consists in finding variable combinations that capture large portions of the variation and covariation in one's dataset. \n",
    "\n",
    "'Variable combinations' here means *linear* combinations. That is, if again $x_{ij}$ denotes the data value for the $i$th observation and the $j$th variable, the value of a principal component is of the form:\n",
    "\n",
    "$$\\text{PC}_{i} = \\sum_j w_j x_{ij} \\quad(\\text{value of PC for observation } i)$$\n",
    "\n",
    "The weights $w_j$ for each variable are called the *loadings*. The loadings tell which variables are most influential (heavily weighted) in each component, and thus offer an indirect picture of which variables are driving variation and covariation in the original data.\n",
    "\n",
    "Here we'll look at how to:\n",
    "* compute the full set of principal components;\n",
    "* determine the variation they capture;\n",
    "* select a subset of principal components for analysis;\n",
    "* and examine the loadings.\n",
    "\n",
    "The data should be normalized before carrying out PCA. (You'll see why a little later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center and scale ('normalize')\n",
    "x_ctr = (x_mx - x_mx.mean())/x_mx.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing PC loadings\n",
    "\n",
    "In `sklearn`, the module `PCA(...)` computes principal components, the proportion of variance captured by each one, and the loadings of each one. The syntax may be a bit different than what you're used to. First we'll configure the module with a fixed number of components to match the number of variables in the dataset and store the result under a separate name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute principal components\n",
    "pca = PCA(n_components = x_ctr.shape[1]) \n",
    "pca.fit(x_ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most quantities you might want to use in PCA can be retrieved as attributes of `pca` after `pca.fit(...)` has been run. In particular:\n",
    "* `.components_` contains the loadings of the principal components;\n",
    "* `.explained_variance_ratio_` contains the proportion of variation and covariation captured by each principal component.\n",
    "\n",
    "You might find it worthwhile to open up the [PCA documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) and keep the 'Attributes' section visible as you're working through the remainder of this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the number of PCs\n",
    "The basic strategy for selecting a number of principal components to work with is to determine how many are needed to capture a large portion of variation and covariation in the original data. This can be done graphically by plotting the variance ratios.\n",
    "\n",
    "Let's start by retrieving the variance ratios for each component. These are stored as the `.explained_variance_ratio_` attribute of `pca`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance ratios\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the components are sorted in descending order of variance ratio -- that means that the first component always captures the most variation and covariation, the second component always captures the secondmost, and so on. For plotting purposes, it will be helpful to store these in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store proportion of variance explained as a dataframe\n",
    "pca_var_explained = pd.DataFrame({'Proportion of variance explained': pca.explained_variance_ratio_})\n",
    "\n",
    "# add component number as a new column\n",
    "pca_var_explained['Component'] = np.arange(1, 23)\n",
    "\n",
    "# print\n",
    "pca_var_explained.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values report the proportion of variance explained *individually* by each component; it is also useful to show the proportion of variance explained *collectively* by a set of components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1 (a)\n",
    "\n",
    "Add a column to `pca_var_explained` called `Cumulative variance explained` that contains the cumulative sum of the proportion of variance explained. For the first component, this new variable should be equal to the value of `Proportion of variance explained`; for the second component, it should be equal to the sum of the values of `Proportion of variance explained` for components 1 and 2; for the third, to the sum of values for components 1, 2, and 3; and so on.\n",
    "\n",
    "Print the first few rows.\n",
    "\n",
    "(*Hint*: use `cumsum(...)` with an appropriate axis specification.)\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_a\n",
    "points: 1\n",
    "manual: false\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add cumulative variance explained as a new column\n",
    "pca_var_explained['Cumulative variance explained'] = pca_var_explained['Proportion of variance explained'].cumsum()\n",
    "\n",
    "# print\n",
    "pca_var_explained.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll make a dual-axis plot showing, on one side, the proportion of variance explained (y) as a function of component (x), and on the other side, the cumulative variance explained (y) also as a function of component (x). Make sure that you've completed Q1(a) before running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode component axis only as base layer\n",
    "base = alt.Chart(pca_var_explained).encode(\n",
    "    x = 'Component')\n",
    "\n",
    "# make a base layer for the proportion of variance explained\n",
    "prop_var_base = base.encode(\n",
    "    y = alt.Y('Proportion of variance explained',\n",
    "              axis = alt.Axis(titleColor = '#57A44C'))\n",
    ")\n",
    "\n",
    "# make a base layer for the cumulative variance explained\n",
    "cum_var_base = base.encode(\n",
    "    y = alt.Y('Cumulative variance explained', axis = alt.Axis(titleColor = '#5276A7'))\n",
    ")\n",
    "\n",
    "# add points and lines to each base layer\n",
    "prop_var = prop_var_base.mark_line(stroke = '#57A44C') + prop_var_base.mark_point(color = '#57A44C')\n",
    "cum_var = cum_var_base.mark_line() + cum_var_base.mark_point()\n",
    "\n",
    "# layer the layers\n",
    "var_explained_plot = alt.layer(prop_var, cum_var).resolve_scale(y = 'independent')\n",
    "\n",
    "# display\n",
    "var_explained_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of making this plot is to quickly determine the fewest number of principal components that capture a considerable portion of variation and covariation. 'Considerable' here is a bit subjective. \n",
    "\n",
    "In this case, we'll base that decision on the proportion of variance explained (left axis) rather than the cumulative variance explained. Notice that there are diminishing gains after a certain number of components, in the sense that adjacent components explain similar proportions of variation. Sometimes it's said that there's an 'elbow' in the plot to describe this phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1 (b)\n",
    "\n",
    "Using the graph and table above, how many principal components explain more than 6% of total variation (variation and covariation) individually? Store this in `main_pca`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_b\n",
    "points: 1\n",
    "manual: false\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_pca = 4\n",
    "\n",
    "#print\n",
    "main_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1 (c)\n",
    "\n",
    "How much total variation is captured collectively by the number of components you stated above? Store this exact proportion in `main_variation`.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_c\n",
    "points: 1\n",
    "manual: false\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_variation = pca_var_explained.loc[0:3,'Proportion of variance explained'].sum()\n",
    "\n",
    "#print\n",
    "main_variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 1 (d)\n",
    "\n",
    "Indicate your selected number of components (answer in Q1(b)) by adding a vertical line to the plot above. Instead of placing the line directly on your selected number of components, put it at the midpoint between your selected number and the next-largest number. Choose a [color of your liking](https://www.w3schools.com/colors/colors_2021.asp) for the line. If you're not sure where to start, have a look at the week 5 lecture codes.\n",
    "\n",
    "(*Hint*: in order to make this work in Altair, you'll need to layer the line on to either `prop_var` or `cum_var` *before* calling `alt.layer(...)`; if you try to add the line as a layer to `var_explained_plot`, Altair will throw an error.)\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_d\n",
    "points: 1\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add vertical line indicating number of selected pcs\n",
    "rule = alt.Chart(pd.DataFrame({'Component': [4.5]})).mark_rule(color='#D2386C').encode(\n",
    "    x='Component')\n",
    "# add line to one layer\n",
    "prop_var += rule\n",
    "# Layer the layers\n",
    "var_explained_plot = alt.layer(prop_var, cum_var).resolve_scale(y = 'independent')\n",
    "\n",
    "# display\n",
    "var_explained_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "### Plotting and interpreting loadings\n",
    "\n",
    "Now that you've chosen the number of components to work with, the next step is to examine loadings to understand just *which* variables the components combine with significant weight.\n",
    "\n",
    "The loadings are stored as the `.components_` attribute of `pca` as an array of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadings for first two pcs\n",
    "pca.components_[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the variance ratios, these will be more useful to us in a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 1 (e)\n",
    "\n",
    "Modify the code cell below to rename and select the loadings for the number of components you chose above.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_e\n",
    "points: 1\n",
    "manual: false\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the loadings as a data frame with appropriate names\n",
    "loading_df = pd.DataFrame(pca.components_).transpose().rename(\n",
    "    columns = {0: 'PC1', 1: 'PC2', 2: 'PC3', 3: 'PC4'} # add entries for each selected component\n",
    ").loc[:, ['PC1','PC2','PC3', 'PC4']] # slice just components of interest\n",
    "\n",
    "# add a column with the variable names\n",
    "loading_df['Variable'] = corr_mx.columns\n",
    "# print\n",
    "loading_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the loadings are the *weights* with which the variables are combined to form the principal components. This is why the variable names have been appended as a separate column: each row is the weight for one variable in the dataset, and each column is a distinct set of weights.\n",
    "\n",
    "For example, the `PC1` column tells us that this component is equal to:\n",
    "\n",
    "$$(0.020055\\times\\text{women}) + (-0.289614\\times\\text{white}) + (-0.050698\\times\\text{citizen}) + \\dots$$\n",
    "\n",
    "Since the components together capture over half the total variation, the heavily weighted variables in the selected components are the ones that drive variation in the original data. By visualizing the loadings, we can see which variables are most influential for each component, and thereby also which variables seem to drive total variation in the data.\n",
    "\n",
    "Loadings are typically plotted against variable name as points connected by lines, as in the plot below. Make sure the previous question is complete before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# melt from wide to long\n",
    "loading_plot_df = loading_df.melt(\n",
    "    id_vars = 'Variable',\n",
    "    var_name = 'Principal Component',\n",
    "    value_name = 'Loading'\n",
    ")\n",
    "\n",
    "# add a column of zeros to encode for x = 0 line to plot\n",
    "loading_plot_df['zero'] = np.repeat(0, len(loading_plot_df))\n",
    "\n",
    "# create base layer\n",
    "base = alt.Chart(loading_plot_df)\n",
    "\n",
    "# create lines + points for loadings\n",
    "loadings = base.mark_line(point = True).encode(\n",
    "    y = alt.X('Variable', title = ''),\n",
    "    x = 'Loading',\n",
    "    color = 'Principal Component'\n",
    ")\n",
    "\n",
    "# create line at zero\n",
    "rule = base.mark_rule().encode(x = alt.X('zero', title = 'Loading'), size = alt.value(0.05))\n",
    "\n",
    "# layer\n",
    "loading_plot = (loadings + rule).properties(width = 120)\n",
    "\n",
    "# show\n",
    "loading_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 1 (f)\n",
    "\n",
    "The plot above is a bit crowded -- use `.facet(...)` to show each line separately. The resulting plot should have four adjacent panels, one for each PC.\n",
    "\n",
    "(*Hint*: you can do this in one line by modifying `loading_plot`.)\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_f\n",
    "points: 1\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loading_plot.facet(column = 'Principal Component:N', columns = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "Great, but what do these plots have to say?\n",
    "\n",
    "Look first at PC1: the variables with the largest loadings (points farthest in either direction from the zero line) are Child Poverty (positive), Employed (negative), Income per capita (negative), Poverty (positive), and Unemployment (positive). We know from exploring the correlation matrix that employment rate, unemployment rate, and income per capita are all related, and similarly child poverty rate and poverty rate are related. Therefore, the positively-loaded variables are all measuring more or less the same thing, and likewise for the negatively-loaded variables. \n",
    "\n",
    "Essentially, then, PC1 is predominantly (but not entirely) a representation of income and poverty. In particular, counties have a higher value for PC1 if they have lower-than-average income per capita and higher-than-average poverty rates, and a smaller value for PC1 if they have higher-than-average income per capita and lower-than-average poverty rates.\n",
    "\n",
    "Often interpreting principal components can be difficult, and sometimes there's no clear interpretation available! That said, it helps to have a system instead of staring at the plot and scratching our heads. Here is a semi-systematic approach to interpreting loadings:\n",
    "0. Divert your attention away from the zero line.\n",
    "1. Find the largest positive loading, and list all variables with similar loadings.\n",
    "2. Find the largest negative loading, and list all variables with similar loadings.\n",
    "3. The principal component represents the difference between the average of the first set and the average of the second set.\n",
    "4. Try to come up with a description of less than 4 words.\n",
    "\n",
    "This system is based on the following ideas:\n",
    "* a high loading value (negative or positive) indicates that a variable strongly influences the principal component;\n",
    "* a negative loading value indicates that\n",
    "    + increases in the value of a variable *decrease* the value of the principal component \n",
    "    + and decreases in the value of a variable *increase* the value of the principal component;\n",
    "* a positive loading value indicates that \n",
    "    + increases in the value of a variable *increase* the value of the principal component\n",
    "    + and decreases in the value of a variable *decrease* the value of the principal component;\n",
    "* similar loadings between two or more variables indicate that the principal component reflects their *average*;\n",
    "* divergent loadings between two sets of variables indicates that the principal component reflects their *difference*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call PC1 'Income and poverty'. Here are my best stabs at the remaining ones.\n",
    "\n",
    "PC2: Self employment. (High values come from high self employment + high work-at-home + low private sector workers.)\n",
    "\n",
    "PC3: Urbanization. (High values come from high transit use + professional/office workers + commute + diversity + high income.)\n",
    "\n",
    "PC4: Carpooling. (?)\n",
    "\n",
    "You'll get some practice with this in HW3. For now, please take a moment to consider how I arrived at these interpretations by looking at the loading plots and thinking through the steps above.\n",
    "\n",
    "### Why normalize?\n",
    "\n",
    "Data are typically normalized because without normalization, the variables on the largest scales tend to dominate the principal components, and most of the time PC1 will capture the majority of the variation.\n",
    "\n",
    "However, that is artificial. In the census data, income per capita has the largest magnitudes, and thus, the highest variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three largest variances\n",
    "x_mx.var().sort_values(ascending = False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When PCs are computed without normalization, the total variation is mostly just the variance of income per capita. But that's just because of the *scale* of the variable -- incomes per capita are large numbers -- not a reflection that it varies more or less than the other variables.\n",
    "\n",
    "Run the cell below to see what happens to the loadings if the data are not normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recompute pcs without normalization\n",
    "pca_unscaled = PCA(22)\n",
    "pca_unscaled.fit(x_mx)\n",
    "\n",
    "# show variance ratios for first three pcs\n",
    "pd.Series(pca_unscaled.explained_variance_ratio_, index = range(1, 23)).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the loadings as a data frame with appropriate names\n",
    "unscaled_loading_df = pd.DataFrame(pca_unscaled.components_).transpose().rename(\n",
    "    columns = {0: 'PC1', 1: 'PC2'} # add entries for each selected component\n",
    ").loc[:, ['PC1', 'PC2']] # slice just components of interest\n",
    "\n",
    "# add a column with the variable names\n",
    "unscaled_loading_df['Variable'] = x_mx.columns.values\n",
    "\n",
    "# melt from wide to long\n",
    "unscaled_loading_plot_df = unscaled_loading_df.melt(\n",
    "    id_vars = 'Variable',\n",
    "    var_name = 'Principal Component',\n",
    "    value_name = 'Loading'\n",
    ")\n",
    "\n",
    "# add a column of zeros to encode for x = 0 line to plot\n",
    "unscaled_loading_plot_df['zero'] = np.repeat(0, len(unscaled_loading_plot_df))\n",
    "\n",
    "# create base layer\n",
    "base = alt.Chart(unscaled_loading_plot_df)\n",
    "\n",
    "# create lines + points for loadings\n",
    "loadings = base.mark_line(point = True).encode(\n",
    "    y = alt.X('Variable', title = ''),\n",
    "    x = 'Loading',\n",
    "    color = 'Principal Component'\n",
    ")\n",
    "\n",
    "# create line at zero\n",
    "rule = base.mark_rule().encode(x = alt.X('zero', title = 'Loading'), size = alt.value(0.05))\n",
    "\n",
    "# layer\n",
    "loading_plot = (loadings + rule).properties(width = 120, title = 'Loadings from unscaled PCA')\n",
    "\n",
    "# show\n",
    "loading_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the variables with nonzero loadings in unscaled PCA are simply the three variables with the largest variances.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# three largest variances\n",
    "x_mx.var().sort_values(ascending = False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Exploratory analysis based on PCA\n",
    "\n",
    "Now that we have the principal components, we can use them for exploratory data visualizations. The principal component values are computed via `.fit_transform(...)` in the PCA module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# principal component values\n",
    "pca.fit_transform(x_ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below extracts the first four PCs and stores them as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project data onto first four components; store as data frame\n",
    "projected_data = pd.DataFrame(pca.fit_transform(x_ctr)).iloc[:, 0:4].rename(columns = {0: 'PC1', 1: 'PC2', 2: 'PC3', 3: 'PC4'})\n",
    "\n",
    "# add state and county\n",
    "projected_data[['State', 'County']] = census[['State', 'County']]\n",
    "\n",
    "# print\n",
    "projected_data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PC's can be used to construct scatterplots of the data and search for patterns. \n",
    "\n",
    "### Outliers\n",
    "The cell below plots PC2 (self-employment) against PC4 (carpooling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base chart\n",
    "base = alt.Chart(projected_data)\n",
    "\n",
    "# data scatter\n",
    "scatter = base.mark_point(opacity = 0.2).encode(\n",
    "    x = alt.X('PC2:Q', title = 'Self-employment PC'),\n",
    "    y = alt.Y('PC4:Q', title = 'Carpooling PC')\n",
    ")\n",
    "\n",
    "# show\n",
    "scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are a handful of outling points in the upper right region away from the dense scatter. What are those?\n",
    "\n",
    "In order to inspect the outlying counties, we first need to figure out how to identify them. The outlying values have a large *sum* of PC2 and PC4. We can distinguish them by finding a cutoff value for the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 2 (a)\n",
    "\n",
    "Compute the sum of principal components 2 and 4 and sort them in descending order. Store the result in `pc_2plus4` and print the first 15 sorted values. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_a\n",
    "points: 1\n",
    "manual: false\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find cutoff value\n",
    "pc_2plus4 = (projected_data['PC2'] + projected_data['PC4'])\n",
    "#print\n",
    "pc_2plus4 = pc_2plus4.sort_values(ascending = False)\n",
    "pc_2plus4 = pd.Series(pc_2plus4)\n",
    "pc_2plus4.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there's a large jump from about 10 to about 13 (you could compare this with the typical jump using `.diff()` if you're curious); so we'll take 12 as the cutoff value. The plot below shows that this cutoff captures the points of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store outlying rows using cutoff\n",
    "outliers = projected_data[(projected_data.PC2 + projected_data.PC4) > 12]\n",
    "\n",
    "# plot outliers in red\n",
    "pts = alt.Chart(outliers).mark_circle(\n",
    "    color = 'red',\n",
    "    opacity = 0.3\n",
    ").encode(\n",
    "    x = 'PC2',\n",
    "    y = 'PC4'\n",
    ")\n",
    "\n",
    "# layer\n",
    "scatter + pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that all the outlying counties are remote regions of Alaska:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What sets them apart? The cell below retrieves the normalized data and county name for the outlying rows, and then plots the normalized values of each variable for all 9 counties as vertical ticks, along with a point indicating the mean for the outlying counties. This plot can be used to determine which variables are over- or under-average for the outlying counties relative to the nation by simply locating means that are far from zero in either direction.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve normalized data for outlying rows\n",
    "outlier_data = x_ctr.loc[outliers.index.values].join(\n",
    "    census.loc[outliers.index.values, ['County']]\n",
    ")\n",
    "\n",
    "# melt to long format for plotting\n",
    "outlier_plot_df = outlier_data.melt(\n",
    "    id_vars = 'County',\n",
    "    var_name = 'Variable',\n",
    "    value_name = 'Normalized value'\n",
    ")\n",
    "\n",
    "# plot ticks for values (x) for each variable (y)\n",
    "ticks = alt.Chart(outlier_plot_df).mark_tick().encode(\n",
    "    x = 'Normalized value',\n",
    "    y = 'Variable'\n",
    ")\n",
    "\n",
    "# shade out region within 3SD of mean\n",
    "grey = alt.Chart(\n",
    "    pd.DataFrame(\n",
    "        {'Variable': x_ctr.columns, \n",
    "         'upr': np.repeat(3, 22), \n",
    "         'lwr': np.repeat(-3, 22)}\n",
    "    )\n",
    ").mark_area(opacity = 0.2, color = 'gray').encode(\n",
    "    y = 'Variable',\n",
    "    x = alt.X('upr', title = 'Normalized value'),\n",
    "    x2 = 'lwr'\n",
    ")\n",
    "\n",
    "# compute means of each variable across counties\n",
    "means = alt.Chart(outlier_plot_df).transform_aggregate(\n",
    "    group_mean = 'mean(Normalized value)',\n",
    "    groupby = ['Variable']\n",
    ").transform_calculate(\n",
    "    large = 'abs(datum.group_mean) > 3'\n",
    ").mark_circle(size = 80).encode(\n",
    "    x = 'group_mean:Q',\n",
    "    y = 'Variable',\n",
    "    color = alt.Color('large:N', legend = None)\n",
    ")\n",
    "\n",
    "# layer\n",
    "ticks + grey + means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2 (b)\n",
    "The two variables that clearly set the outlying counties apart from the nation are the percentage of the population using alternative transportation (extremely above average) and the percentage that drive to work (extremely below average). Why is this?\n",
    "\n",
    "(*Hint:* take a peek at the [Wikipedia page on transportation in Alaska](https://en.wikipedia.org/wiki/Transportation_in_Alaska).)\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_b\n",
    "points: 1\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Alaska is VERY BIG so there is not one way to which getting around is the easiest. This means people have to find which ever way works best for them to get around that is not driving_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "### Regional patterns\n",
    "Are there regional patterns in the data? The cell below merges a table of U.S. census regions with the projected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add US region\n",
    "regions = pd.read_table('data/regions.txt', sep = ',')\n",
    "plot_df = pd.merge(projected_data, regions, how = 'left', on = 'State')\n",
    "\n",
    "# any non-matches?\n",
    "plot_df.Region.isna().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are some counties that didn't get a match in the region table. In fact, all of Puerto Rico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect rows with missing region\n",
    "plot_df[plot_df.Region.isna()].State.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's an easy fix. We'll just give PR its own epynomous region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaNs\n",
    "plot_df.Region = plot_df.Region.fillna('Puerto Rico')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2 (c)\n",
    "\n",
    "Use `plot_df` to construct a faceted scatterplot of PC2 against PC1 by region, and color the points by region.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_c\n",
    "points: 1\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base chart\n",
    "base = alt.Chart(plot_df)\n",
    "\n",
    "# data scatter\n",
    "scatter = base.mark_point(opacity = 0.2).encode(\n",
    "    x = alt.X('PC2:Q', title = 'Region'),\n",
    "    y = alt.Y('PC1:Q', title = 'Carpooling PC'),\n",
    "    color = 'Region'\n",
    ")\n",
    "scatter.facet(column = 'Region', columns = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2 (d)\n",
    "\n",
    "How does the northeast compare with the south? \n",
    "\n",
    "#### (i) Describe the location of scatter along the PC axes.\n",
    "\n",
    "(For instance, the western region scatter is centered around a PC1 value just below zero, say around -1, and a PC2 value just above zero, say around 2.)\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_d_i\n",
    "points: 1\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "*The northeast and south are very similar on the x axis PC, but on the y axis PC we see the south is centered higher than the north east  .*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### (ii) Can you interpret the difference in location in any way?\n",
    "\n",
    "State one qualitative difference in southern and northeastern counties that this points to.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2_d_ii\n",
    "points: 1\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    "*South is way more open area where walking cities are not a thing, where as north east has walking cities like NYC where driving is not as popular and people instead take the subway.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "## Submission Checklist\n",
    "1. Save file to confirm all changes are on disk\n",
    "2. Run *Kernel > Restart & Run All* to execute all code from top to bottom\n",
    "3. Save file again to write any new output to disk\n",
    "4. Select *File > Download as > HTML*.\n",
    "5. Open in Google Chrome and print to PDF on A3 paper in portrait orientation.\n",
    "6. Submit to Gradescope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
